# templates/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "flink-cdc.fullname" . }}-config
  labels:
    {{- include "flink-cdc.labels" . | nindent 4 }}
data:
  flink-conf.yaml: |
    {{- range $key, $value := .Values.flinkConfiguration }}
    {{ $key }}: {{ $value | quote }}
    {{- end }}
    
    # Auto-configured settings
    jobmanager.rpc.address: {{ include "flink-cdc.fullname" . }}-jobmanager
    taskmanager.numberOfTaskSlots: {{ .Values.taskmanager.slots | quote }}
    
  log4j-console.properties: |
    # This affects logging for both user code and Flink
    rootLogger.level = {{ .Values.logging.logLevel }}
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name = org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
  cdcgateway-config.yaml: |
    adminPort: {{ .Values.cdcGateway.configuration.adminPort }}
    maxConnections: {{ .Values.cdcGateway.configuration.maxConnections }}
    maxConcurrentJobs: {{ .Values.cdcGateway.configuration.maxConcurrentJobs }}
    flinkJobmanager: "{{ include "flink-cdc.fullname" . }}-jobmanager:{{ .Values.jobmanager.ports.rpc }}"
    stateBackend: "{{ .Values.flinkConfiguration.state.backend }}"
    checkpointsDir: "{{ .Values.flinkConfiguration.state.checkpoints.dir }}"
    savepointsDir: "{{ .Values.flinkConfiguration.state.savepoints.dir }}"
    s3Endpoint: "{{ .Values.externalServices.minio.endpoint }}"
    connectorJarsPath: "/opt/flink/lib"
    
  pipeline-definitions.json: |
    {
      "pipelines": [
      {{- range $index, $pipeline := .Values.pipelines }}
      {{- if $pipeline.enabled }}
        {{- if $index }},{{ end }}
        {
          "name": "{{ $pipeline.name }}",
          "source": {
            "type": "{{ $pipeline.source.type }}",
            "config": {
              "hostname": "{{ $.Values.externalServices.mssql.host }}",
              "port": "{{ $.Values.externalServices.mssql.port }}",
              "username": "${MSSQL_USERNAME}",
              "password": "${MSSQL_PASSWORD}",
              "database-name": "{{ $.Values.externalServices.mssql.database }}",
              "table-name": "{{ $pipeline.source.table }}"
              {{- range $key, $value := $pipeline.source.options }}
              ,"{{ $key }}": "{{ $value }}"
              {{- end }}
            }
          },
          "sink": {
            "type": "{{ $pipeline.sink.type }}",
            "config": {
              "bootstrapServers": "{{ $.Values.externalServices.kafka.brokers }}",
              "topic": "{{ $pipeline.sink.topic }}"
              {{- if $.Values.externalServices.kafka.useSASL }}
              ,"sasl.mechanism": "PLAIN",
              ,"security.protocol": "SASL_PLAINTEXT",
              ,"sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"${KAFKA_USERNAME}\" password=\"${KAFKA_PASSWORD}\";"
              {{- end }}
              {{- range $key, $value := $pipeline.sink.options }}
              ,"{{ $key }}": "{{ $value }}"
              {{- end }}
            }
          }
        }
      {{- end }}
      {{- end }}
      ]
    }
---